h1. About

Flay is a tool written by Ryan Davis that calculates a complexity score for methods in your codebase.  The score is based on a published Assignments, Branches, Conditionals (ABC) metric, but has been customised by Ryan to highlight things that he feels are relevant.  It uses ParseTree under the covers to create Sexp nodes for Ruby code, then traverses those nodes to count the assignments, branches and conditionals (and a couple of other things) per method in your code.  Those values are plugged into a formula to come up with a flog score for your code.  The flog scores across all methods are summed to give you a total score for your codebase and an average per method.

h1. Command

find lib app -name "*.rb" | xargs flog

h1. Interface

The default command seems to be interactive, allowing you to type ruby code directly into a console session for analysis.  You can get some information about the options by running:

$ flog --help
Usage: flog [options]
    -a, --all                        display all flog results, not top 60%
    -s, --score                      display total score only
    -m, --methods-only               skip code outside of methods
    -n, --no-details                 skip method details in report
    -c, --continue                   continue despite syntax errors
    -I path1,path2,path3             ruby include paths to search
    -b, --blame                      include blame information for methods
    -v, --verbose                    verbosely display progress and errors
    -q, --quiet                      quiet, don't show method breakdowns
    -h, --help                       show this message

The simplest way to run the tool calling 'flog' and passing a list of files to process.  The top 60% of results (an option is available to show all), in order, go to standard output in the following form:

Total Flog = <total score> (<average score> flog / method)
<class name>#<method name>: (<method score>)
  <score>: <element being scored>
  <score>: <element being scored>
  ...
<class name>#<method name>: (<method score>)
...

For example:

Total Flog = 233.7 (116.9 flog / method)
Monitor::ClientsController#create: (120.9)
    62.4: assignment
    44.8: params
    38.4: []
     7.2: save
     7.2: new

h1. Documentation

The website for Flay is at http://ruby.sadi.st/Flog.html.  It describes a simple command, plus an example of how the algorithm works.  The command line help for Flog lists several useful options for Flog.

h1. Code

The code for flog is mostly in a single class in a single file with about 540 lines of code.  Options parsing is in the bin/flog script, and a gauntlet plugin is in a separate file.  The main class has multiple concerns (eg. file parsing, metric collection, reporting) that could be split into separate classes to improve the design somewhat.  For the most part though, the internal methods are well factored and it's pretty straight forward to see what is going on.

h1. General Comments

Flog is very simple to run and quickly get a feel for where some of the most complex methods in your system are.  The output being ordered based on the complexity of the code is also useful, as you can chip away at them in order.  I'd like to see the order going from least complex to most complex (it's currently the other way around) so that the most complex method was right in front of you in your terminal output when it finishes running.  That saves you scrolling back in your buffer to find the start of the command.  Outputs to text files or HTML should stay in the current order so they're at the top when opened.

[[ "top" methods are the worst. I like that metaphor. I am often
interested in the "top 20" worst methods and `head` does that quite
nicely. `flog -q . | head`. I also run almost all my shells w/in
emacs, so I have superior navigability... Asking eric (who is a poor
vim user, bless his soul), he either redirects to a file or pipes
through more `flog . | more`. ]]

The breakdown of the complexity within the methods is not particularly helpful to me.  I tend to ignore the numbers assigned by flay once I fine the method, because I can see ways to refactor it to make it less complex.  I tend to refactor then run the tool again to see if the complexity has dropped.

[[ no, it isn't... I'm thinking -q should be the default and -v should
show said complexity ]]

Flog has no rake task support out of the box.  Failing the build when scores above some threshold appear seems like an obvious addition.  There is no Rails plugin out of the box either, which might provide a rake task that knew to look inside 'app' and 'lib', but to ignore 'vendor' for code.

[[ again, I won't do a rails plugin. that just doesn't make sense to
me. ]]

[[ rake task has been created and will go out in the next release.
good suggestion. thanks. also, both the flog and flay rake tasks have
been integrated into hoe, so everyone developing with hoe
automatically gets these for free (assuming they have flog and flay
installed) ]]

I have two gripes with Flog.

The first is that the code deviates from the published definition of the ABC metric.  Scores for various types of calls are assigned arbitrarily in the code.  For example, a comment in the code suggests that the Ryan doesn't like people calling 'inject', so he has given it a higher than normal score.  It therefore produces a number that basically tells you what Ryan might think of your code.  It's helpful, but doesn't particularly mean anything scientific.

[[ Who the fuck made the claim that flog was scientific? HOWEVER, it
is anything but arbitrary.

Just to be clear:

    arbitrary: based on random choice or personal whim, rather than
               any reason or system

and before you can object to that:

    whim: a sudden desire or change of mind, esp. one that is unusual
          or unexplained

(oxford american dictionary)

This is ANYTHING but arbitrary. This is based on many years of
experience developing in many projects and companies, large and small.
It is based on the amount of code I've ripped out of slow, bloated,
undecipherable projects while consulting. It is based on years of
teaching ruby both in the classroom and out.

define_method and all forms of eval? more complex...

include? reflective methods? dynamic message sends? more complex...
but less so than the eval forms above, certainly.

alias_method, alias, and singleton class definitions? more complex. 

literals? name a software engineering book that doesn't point out that
using literal values directly in your code is a great idea...

and inject? For every proper functionally reductive use of inject I'll
find you 100 that are more complex, slower, and harder to read forms
of their #each counterparts. It is one of the most misused methods in
ruby, bar-none. Do the benchmarks, it deserves to be penalized.
]]

The second gripe isn't really about Flog, but more about the lack of equally powerful tools for other metrics.  The ABC metric doesn't correlate very well at a detailed level with issues the community talks about.  I'd love to see a similar tool measuring cyclomatic complexity (but see notes on Saikuro) and NPath complexity.  Cyclomatic complexity counts the number of branches in your code, which gives you an indicator of the minimum number of tests you should write.  NPath complexity gives you a count of the total number of possible paths through your code, which gives you an indicator of the number of tests you could write to get complete coverage on your code.  Both of those things seem more pertinent to the community to me.

[[
If ABC metrics don't correlate very well with the issues the community
is talking about, maybe it is because the community is focusing on the
wrong stuff. Far too much emphasis is put on writing clevar and magic
code rather than clear, clean, communicative, maintainable, fast, and
testable code. This comes down from the community leaders themselves.
It comes down from the bloggers. It comes down from everywhere. This I
believe to be true and is the main reason why I give talks at
conferences.

Further, and perhaps of more value, I disagree that flog scores are
not pertinent. I would challenge you to demonstrate significant
negative divergance between cyclomatic complexity and flog's variant
of ABC where real world testing is involved. If anything, you'll find
that cyclomatic complexity diverges as it only accounts for complexity
at a functional mathmatical level. It too is more than naive,
especially about ruby. Flog understands ruby pretty well, and comes
from and is directly influenced by testing challenges that we face. It
even accounts for things like law of demeter violation by penalizing
chained method sends. This is something that both cyclomatic and npath
analysis entirely ignore.

P.S. Look at those scores below. Find me a metric tool besides flog
that'll score your DSL blocks like rake tasks or rspec specs.
]]

h1. The 'Eat Your Own Dog Food' check

$ find /Library/Ruby/Gems/1.8/gems/flog-2.0.0 -name "*.rb" | xargs flog --quiet
Total Flog = 5097.1 (12.0 flog / method)
before#each: (280.7)
main#none: (263.1)
FlogGauntlet#display_report: (202.1)
InstanceMethods#none: (104.1)
ClassMethods#spawn: (96.0)
it#should exit with status 0: (79.8)
InstanceMethods#containing?: (77.7)
ClassMethods#find_with_range_restrictions: (69.7)
it#should create a Flog instance: (68.4)
FlogGauntlet#score_for: (65.9)
GemUpdater#update_gem_tarballs: (64.5)
Flog#process_iter: (61.9)
ClassMethods#generator_for: (59.7)
ClassMethods#with_contained_scope: (58.1)
InstanceMethods#contained_by?: (52.1)
ClassMethods#acts_as_date_range_param_sequentialize_class: (50.3)
ClassMethods#validates_interval: (47.6)
main#score_for: (45.5)
ClassMethods#with_containing_scope: (41.5)
Flog#flog: (41.2)
title#Statistics: (41.1)
Flog#none: (38.8)
it#should output call details for each call for the method: (37.7)
ClassMethods#acts_as_date_range_singleton_sequentialize_class: (34.9)
InstanceMethods#destroy_without_callbacks: (31.2)
it#should output the overall total for the method: (30.2)
ClassMethods#with_current_time_scope: (27.8)
FlogGauntlet#group_by_owner: (27.7)
FlogGauntlet#report_bad_people: (26.8)
Flog#output_method_details: (26.3)
main#report_bad_people: (25.1)
InstanceMethods#overlapping?: (25.0)
it#should compute the sqrt of summed squares for assignments, branches, and other tallies: (24.2)
ClassMethods#gather_exemplars: (23.9)
it#should not take any arguments: (23.6)
it#should update the class stack when recursing: (23.2)
ObjectDaddy#included: (23.0)
it#when it is done it should restore the class stack to its original value: (22.6)
ClassMethods#none: (22.3)
it#should return a ParseTree instance: (22.2)
Flog#score_method: (22.1)
InstanceMethods#add_has_many_range_extension: (22.0)
FlogGauntlet#report_worst: (21.5)
GemUpdater#get_source_index: (21.3)
currently#should compute the same call data as flog-1.1.0: (21.1)
Flog#output_details: (20.9)
FlogGauntlet#report: (20.7)
it#should not fail when flogging the given input: (20.7)
ClassMethods#with_overlapping_scope: (20.4)
main#report_worst: (20.2)
it#should recursively analyze the provided code block: (19.8)
it#should only output details for methods until the threshold is reached: (19.5)
main#report: (19.3)
Flog#flog_file: (18.1)
ClassMethods#acts_as_range_configure_class: (17.9)
it#should fail: (17.8)
it#should not raise an exception: (17.8)
ClassMethods#ranged_lookup: (17.5)
it#should use the multiplier when updating the current call score: (16.9)
it#should ensure that new recorded calls will get 0 counts without explicit initialization: (16.4)
Flog#process_block_pass: (16.0)
InstanceMethods#initialize_with_has_many_range_extension: (15.8)
it#should require a block during which the class name is in effect: (15.7)
it#should not process the failing code: (15.7)
Array#sample_variance: (15.7)
Range#included: (15.7)
DateRange#included: (15.7)
Flog#report: (15.6)
Ranged#none: (15.6)
InstanceMethods#lifetime: (15.5)
BotSender#deliver: (15.3)
it#should return the total flog score divided by the number of calls: (15.1)
it#should set the option to show all methods: (15.0)
it#should set the option to show only the score: (15.0)
it#should set the option to provide 'blame' information: (15.0)
it#should set the option to report on methods only: (15.0)
it#should set the option to be verbose: (15.0)
Flog#klass_name: (15.0)
currently#should compute the same totals data as flog-1.1.0: (15.0)

Flog will show the top 60% of the most complex code, so we always expect some output here.  The average of 12 per method seems excellent to me.  A few of the topmost entries in the list (I'd suggest things with a score of about 50 or higher) probably need some work though.

[[
Not entirely fair. I didn't write those specs and I'm having a bitch
of a time reworking them. If you flog bin/flog and lib/flog.rb you'll
see a flog score of just under 600 (and oddly, the average is still
12).

(gauntlet_flog is really more a part of gauntlet than a part of flog,
but it made sense for me to package and maintain it with flog)
]]
